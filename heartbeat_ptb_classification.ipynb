{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from pyts.image import GramianAngularField\n",
    "from pyts.datasets import load_gunpoint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import resample\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks, NearMiss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import alexnet, vgg16, resnet152, resnet18, vgg19\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hb_normal = pd.read_csv('ptbdb_normal.csv', header=None)\n",
    "df_hb_abnormal = pd.read_csv('ptbdb_abnormal.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hb = pd.concat([df_hb_normal, df_hb_abnormal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hb = df_hb.reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_hb.iloc[:,:187]\n",
    "y = df_hb[187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasf = GramianAngularField(image_size=150, method='difference')\n",
    "x_gasf_train = gasf.fit_transform(x_train)\n",
    "x_gasf_test = gasf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hb_data_ptb.hdf5', mode='w') as hdf5_file:\n",
    "    hdf5_file.create_dataset(\"x_train\", (len(x_gasf_train), 224, 224), np.float32)\n",
    "    hdf5_file.create_dataset(\"y_train\", (len(y_train),), np.int32)\n",
    "    hdf5_file.create_dataset(\"x_test\", (len(x_gasf_test), 224, 224), np.float32)\n",
    "    hdf5_file.create_dataset(\"y_test\", (len(y_test),), np.int32)\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(x_gasf_train, y_train)):\n",
    "        image = Image.fromarray(x.astype(float))\n",
    "        t = np.array(transform(image))\n",
    "        hdf5_file[\"x_train\"][i, ...] = t\n",
    "        hdf5_file[\"y_train\"][i] = y\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_gasf_test, y_test)):\n",
    "        image = Image.fromarray(x.astype(float))\n",
    "        t = np.array(transform(image))\n",
    "        hdf5_file[\"x_test\"][i, ...] = t\n",
    "        hdf5_file[\"y_test\"][i] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrence_plot(s, eps=None, steps=None):\n",
    "    result = []\n",
    "    if eps==None: eps=0.1\n",
    "    if steps==None: steps=10\n",
    "    d = pairwise_distances(s[:, None])\n",
    "    d = d / eps\n",
    "    d[d > steps] = steps\n",
    "    return d/5. - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hb_data_ptb_rp.hdf5', mode='w') as hdf5_file:\n",
    "    hdf5_file.create_dataset(\"x_train\", (len(x_train), 224, 224), np.float32)\n",
    "    hdf5_file.create_dataset(\"y_train\", (len(y_train),), np.int32)\n",
    "    hdf5_file.create_dataset(\"x_test\", (len(x_test), 224, 224), np.float32)\n",
    "    hdf5_file.create_dataset(\"y_test\", (len(y_test),), np.int32)\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(x_train.values, y_train)):\n",
    "        r = recurrence_plot(x, steps=10)\n",
    "        image = Image.fromarray(r.astype(float))\n",
    "        t = np.array(transform(image))\n",
    "        hdf5_file[\"x_train\"][i, ...] = t\n",
    "        hdf5_file[\"y_train\"][i] = y\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_test.values, y_test)):\n",
    "        r = recurrence_plot(x, steps=10)\n",
    "        image = Image.fromarray(r.astype(float))\n",
    "        t = np.array(transform(image))\n",
    "        hdf5_file[\"x_test\"][i, ...] = t\n",
    "        hdf5_file[\"y_test\"][i] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hb_data_ptb_mixed.hdf5', mode='w') as hdf5_file:\n",
    "    hdf5_file.create_dataset(\"x_train\", (len(x_train), 3, 224, 224), np.float32)\n",
    "    hdf5_file.create_dataset(\"y_train\", (len(y_train),), np.int32)\n",
    "    hdf5_file.create_dataset(\"x_test\", (len(x_test), 3, 224, 224), np.float32)\n",
    "    hdf5_file.create_dataset(\"y_test\", (len(y_test),), np.int32)\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    \n",
    "    for i, (x1, x2, y) in enumerate(zip(x_gasf_train, x_train.values, y_train)):\n",
    "        image = Image.fromarray(x1.astype(float))\n",
    "        t = np.array(transform(image))\n",
    "        r = recurrence_plot(x2, steps=10)\n",
    "        image = Image.fromarray(r.astype(float))\n",
    "        t2 = np.array(transform(image))\n",
    "        hdf5_file[\"x_train\"][i, 0, ...] = t\n",
    "        hdf5_file[\"x_train\"][i, 1, ...] = t2\n",
    "        hdf5_file[\"x_train\"][i, 2, ...] = (t + t2) / 2\n",
    "        hdf5_file[\"y_train\"][i] = y\n",
    "\n",
    "    for i, (x1, x2, y) in enumerate(zip(x_gasf_test, x_test.values, y_test)):\n",
    "        image = Image.fromarray(x1.astype(float))\n",
    "        t = np.array(transform(image))\n",
    "        r = recurrence_plot(x2, steps=10)\n",
    "        image = Image.fromarray(r.astype(float))\n",
    "        t2 = np.array(transform(image))\n",
    "        hdf5_file[\"x_test\"][i, 0, ...] = t\n",
    "        hdf5_file[\"x_test\"][i, 1, ...] = t2\n",
    "        hdf5_file[\"x_test\"][i, 2, ...] = (t + t2) / 2\n",
    "        hdf5_file[\"y_test\"][i] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading checkpoint from MIT BIH model\n",
    "ckpt_recall = torch.load('res_net_test_recall_best_cmb.chk')\n",
    "res_net_saved_recall = resnet18(pretrained=True)\n",
    "for param in res_net_saved_recall.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# MIT BIH model had 2 frozen convolutional layers, 3 trainable FC layers\n",
    "# In this model all convolutional layers are frozen but 3 FCs are trainable\n",
    "# The idea is that the feature generator from MIT BIH will be frozen\n",
    "num_ftrs = res_net_saved_recall.fc.in_features\n",
    "res_net_saved_recall.fc = nn.Sequential(\n",
    "                nn.Linear(in_features=num_ftrs, out_features=256, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=5, bias=True))\n",
    "res_net_saved_recall.load_state_dict(ckpt_recall['net'])\n",
    "res_net_saved_recall.fc = nn.Sequential(\n",
    "                nn.Linear(in_features=num_ftrs, out_features=256, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=2, bias=True))\n",
    "res_net_saved_recall.cuda()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for moving tensor or model to GPU\n",
    "def cuda(xs):\n",
    "    if torch.cuda.is_available():\n",
    "        if not isinstance(xs, (list, tuple)):\n",
    "            return xs.cuda()\n",
    "        else:\n",
    "            return [x.cuda() for x in xs]\n",
    "    else:\n",
    "        return xs\n",
    "\n",
    "# Custom class for defining dataset for training with augmentation\n",
    "class Dataset_Hdf5(Dataset):\n",
    "\n",
    "    def __init__(self, path, data_type):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.file = h5py.File(path, 'r')\n",
    "        self.images = self.file['x_{}'.format(data_type)]\n",
    "        self.labels = self.file['y_{}'.format(data_type)]\n",
    "                \n",
    "        self.len = self.images.shape[0]\n",
    "        if data_type == 'train':\n",
    "            self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        # unsqueeze adds dimension to image -> converts to 1x224x224 since we don't have rgb\n",
    "        return self.transform(self.images[index].astype('float32')), \\\n",
    "                torch.tensor(self.labels[index], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "    \n",
    "# Custom class for defining dataset for training with augmentation\n",
    "class Dataset_Hdf5_3C(Dataset):\n",
    "\n",
    "    def __init__(self, path, data_type):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.file = h5py.File(path, 'r')\n",
    "        self.images = self.file['x_{}'.format(data_type)]\n",
    "        self.labels = self.file['y_{}'.format(data_type)]\n",
    "                \n",
    "        self.len = self.images.shape[0]\n",
    "\n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        # unsqueeze adds dimension to image -> converts to 1x224x224 since we don't have rgb\n",
    "        return torch.tensor(self.images[index].astype('float32')), \\\n",
    "                torch.tensor(self.labels[index], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_train_loader = torch.utils.data.DataLoader(Dataset_Hdf5('/home/asif/heartbeat/hb_data_ptb_rp.hdf5', 'train'), \n",
    "                                                batch_size=64, shuffle=True)\n",
    "hb_test_loader = torch.utils.data.DataLoader(Dataset_Hdf5('/home/asif/heartbeat/hb_data_ptb_rp.hdf5', 'test'), \n",
    "                                                batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_train_loader2 = torch.utils.data.DataLoader(Dataset_Hdf5_3C('/home/asif/heartbeat/hb_data_ptb_mixed.hdf5', 'train'), \n",
    "                                                batch_size=64, shuffle=True)\n",
    "hb_test_loader2 = torch.utils.data.DataLoader(Dataset_Hdf5_3C('/home/asif/heartbeat/hb_data_ptb_mixed.hdf5', 'test'), \n",
    "                                                batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = cuda(torch.tensor([2.0, 1.0]))\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_res_net = torch.optim.Adam([\n",
    "                                          {\"params\": res_net_saved_recall.fc[0].parameters(), \"lr\": 0.001},\n",
    "                                          {\"params\": res_net_saved_recall.fc[2].parameters(), \"lr\": 0.001},\n",
    "                                          {\"params\": res_net_saved_recall.fc[4].parameters(), \"lr\": 0.001},\n",
    "                                           ],  \n",
    "                                lr=0.0001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, criterion, optimizer, test_loader, num_epochs=30):\n",
    "    net.train()\n",
    "    train_acc_max = 0\n",
    "    test_acc_max = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "    \n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        net.train()\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "   \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = cuda(data)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            # outputs = net(inputs.expand(-1, 3, -1, -1))\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print('End of epoch {}, Loss {}'.format(epoch + 1, running_loss / len(train_loader)))\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        print('Train accuracy: {}'.format(train_acc))\n",
    "        test_acc, all_true, all_pred = test(net, test_loader)\n",
    "        print('Test accuracy: {}'.format(test_acc))\n",
    "        precision = precision_score(all_true, all_pred, average='macro')\n",
    "        recall = recall_score(all_true, all_pred, average='macro')\n",
    "        f1 = f1_score(all_true, all_pred, average='macro')\n",
    "        print('Test precision: {}'.format(precision))\n",
    "        print('Test recall: {}'.format(recall))\n",
    "        print('Test f1: {}'.format(f1))\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "def test(net, test_loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            images, labels = cuda(data)\n",
    "            all_true.extend(labels.cpu().tolist())\n",
    "#             outputs = net(images.expand(-1, 3, -1, -1))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_pred.extend(predicted.cpu().tolist())\n",
    "#             predicted = predicted.float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = correct / total\n",
    "#     print('Accuracy of the network on the images: %d %%' % (100 * acc))\n",
    "    return acc, all_true, all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, Loss 0.5297282012281838\n",
      "Train accuracy: 0.7288033674082982\n",
      "Test accuracy: 0.7475094469254552\n",
      "Test precision: 0.7328944646659404\n",
      "Test recall: 0.7886693936788673\n",
      "Test f1: 0.7295574334876124\n",
      "End of epoch 2, Loss 0.45415015575977474\n",
      "Train accuracy: 0.776393780603041\n",
      "Test accuracy: 0.8110614908965991\n",
      "Test precision: 0.7666388169070044\n",
      "Test recall: 0.7494160602828079\n",
      "Test f1: 0.757004015303564\n",
      "End of epoch 3, Loss 0.4117244554089976\n",
      "Train accuracy: 0.810239670131432\n",
      "Test accuracy: 0.8392305049811062\n",
      "Test precision: 0.7992057175402435\n",
      "Test recall: 0.8373533829103837\n",
      "Test f1: 0.812732993021481\n",
      "End of epoch 4, Loss 0.37393098725722385\n",
      "Train accuracy: 0.8356670389141826\n",
      "Test accuracy: 0.8570937822054276\n",
      "Test precision: 0.8183661723046516\n",
      "Test recall: 0.8493423768522297\n",
      "Test f1: 0.8307207998076526\n",
      "End of epoch 5, Loss 0.36926299243510424\n",
      "Train accuracy: 0.8319731981788506\n",
      "Test accuracy: 0.8461009962212298\n",
      "Test precision: 0.8258977983109367\n",
      "Test recall: 0.7725381324984505\n",
      "Test f1: 0.7923548218630186\n",
      "End of epoch 6, Loss 0.32925393669814856\n",
      "Train accuracy: 0.857486470234516\n",
      "Test accuracy: 0.8722088629336998\n",
      "Test precision: 0.8355595084261667\n",
      "Test recall: 0.8674121649991355\n",
      "Test f1: 0.8484302356829969\n",
      "End of epoch 7, Loss 0.32912645063229967\n",
      "Train accuracy: 0.8522463705867194\n",
      "Test accuracy: 0.8660254208175885\n",
      "Test precision: 0.8298925478019283\n",
      "Test recall: 0.8798583725664768\n",
      "Test f1: 0.8461189593738547\n",
      "End of epoch 8, Loss 0.31807686806052593\n",
      "Train accuracy: 0.861438020788592\n",
      "Test accuracy: 0.880109927859842\n",
      "Test precision: 0.8449709212179902\n",
      "Test recall: 0.8728831450181651\n",
      "Test f1: 0.856727276829816\n",
      "End of epoch 9, Loss 0.2989256664455592\n",
      "Train accuracy: 0.8700283480800618\n",
      "Test accuracy: 0.8667124699416008\n",
      "Test precision: 0.8323352452065174\n",
      "Test recall: 0.889458388561603\n",
      "Test f1: 0.848838317326738\n",
      "End of epoch 10, Loss 0.2779145413538912\n",
      "Train accuracy: 0.8808521604673138\n",
      "Test accuracy: 0.8945379594641016\n",
      "Test precision: 0.8636714036170835\n",
      "Test recall: 0.881733095445035\n",
      "Test f1: 0.8718879488857463\n",
      "End of epoch 11, Loss 0.27821084070991686\n",
      "Train accuracy: 0.8812816768318873\n",
      "Test accuracy: 0.8976296805221573\n",
      "Test precision: 0.8712209559456805\n",
      "Test recall: 0.875129813386274\n",
      "Test f1: 0.8731401212036127\n",
      "End of epoch 12, Loss 0.2689893488045577\n",
      "Train accuracy: 0.8836010652005841\n",
      "Test accuracy: 0.896942631398145\n",
      "Test precision: 0.8631342660814544\n",
      "Test recall: 0.9043079814503581\n",
      "Test f1: 0.8789987363660547\n",
      "End of epoch 13, Loss 0.2644710114614649\n",
      "Train accuracy: 0.8866076797525986\n",
      "Test accuracy: 0.8986602542081759\n",
      "Test precision: 0.8703661843194337\n",
      "Test recall: 0.8815460936020671\n",
      "Test f1: 0.87565983924805\n",
      "End of epoch 14, Loss 0.27707102088319074\n",
      "Train accuracy: 0.882484322652693\n",
      "Test accuracy: 0.9127447612504294\n",
      "Test precision: 0.8854869492001123\n",
      "Test recall: 0.9038445932357082\n",
      "Test f1: 0.8938931678929659\n",
      "End of epoch 15, Loss 0.2523145760808672\n",
      "Train accuracy: 0.8930504252212009\n",
      "Test accuracy: 0.9117141875644108\n",
      "Test precision: 0.8852026170878444\n",
      "Test recall: 0.9000895609455472\n",
      "Test f1: 0.8921435441064507\n",
      "End of epoch 16, Loss 0.24380310100841\n",
      "Train accuracy: 0.8973455888669358\n",
      "Test accuracy: 0.9113706630024047\n",
      "Test precision: 0.8848664343786294\n",
      "Test recall: 0.8994715139739773\n",
      "Test f1: 0.8916851268561143\n",
      "End of epoch 17, Loss 0.24984503668415678\n",
      "Train accuracy: 0.895026200498239\n",
      "Test accuracy: 0.9062177945723119\n",
      "Test precision: 0.8746813805041653\n",
      "Test recall: 0.9073088317794931\n",
      "Test f1: 0.8882960039300767\n",
      "End of epoch 18, Loss 0.24296276815808737\n",
      "Train accuracy: 0.8957134266815565\n",
      "Test accuracy: 0.895225008588114\n",
      "Test precision: 0.8615745695850279\n",
      "Test recall: 0.9171852341463014\n",
      "Test f1: 0.8798272695071314\n",
      "End of epoch 19, Loss 0.22755458676716783\n",
      "Train accuracy: 0.9041319474271969\n",
      "Test accuracy: 0.9117141875644108\n",
      "Test precision: 0.8804097442958807\n",
      "Test recall: 0.9171975833246105\n",
      "Test f1: 0.8953876439381767\n",
      "End of epoch 20, Loss 0.2209459048296724\n",
      "Train accuracy: 0.9099733699853965\n",
      "Test accuracy: 0.8873239436619719\n",
      "Test precision: 0.853310853326838\n",
      "Test recall: 0.90943318447673\n",
      "Test f1: 0.8711627103641093\n",
      "End of epoch 21, Loss 0.2248838973733095\n",
      "Train accuracy: 0.906107722704235\n",
      "Test accuracy: 0.9172105805565098\n",
      "Test precision: 0.8900239667809269\n",
      "Test recall: 0.9114990255910258\n",
      "Test f1: 0.8997105899490585\n",
      "End of epoch 22, Loss 0.21565796574065974\n",
      "Train accuracy: 0.9087707241645907\n",
      "Test accuracy: 0.9055307454482996\n",
      "Test precision: 0.8724804505104908\n",
      "Test recall: 0.9193789774645138\n",
      "Test f1: 0.8898610228475174\n",
      "End of epoch 23, Loss 0.2111920224768775\n",
      "Train accuracy: 0.9097156601666524\n",
      "Test accuracy: 0.9020954998282377\n",
      "Test precision: 0.8686794510474097\n",
      "Test recall: 0.9150993991242669\n",
      "Test f1: 0.8858559691328818\n",
      "End of epoch 24, Loss 0.20809642250066276\n",
      "Train accuracy: 0.9138390172665578\n",
      "Test accuracy: 0.9151494331844727\n",
      "Test precision: 0.8841149048570424\n",
      "Test recall: 0.9226176964901283\n",
      "Test f1: 0.8996513000013258\n",
      "End of epoch 25, Loss 0.20036622533922668\n",
      "Train accuracy: 0.9182200841852075\n",
      "Test accuracy: 0.9206458261765716\n",
      "Test precision: 0.9017244697254119\n",
      "Test recall: 0.9001912946525705\n",
      "Test f1: 0.9009530989081496\n",
      "End of epoch 26, Loss 0.21354686579370236\n",
      "Train accuracy: 0.913237694356155\n",
      "Test accuracy: 0.923394022672621\n",
      "Test precision: 0.8990423542130308\n",
      "Test recall: 0.915020305577477\n",
      "Test f1: 0.906479312091997\n",
      "End of epoch 27, Loss 0.19716024861394704\n",
      "Train accuracy: 0.9202817627351602\n",
      "Test accuracy: 0.9271727928546891\n",
      "Test precision: 0.9021683823241614\n",
      "Test recall: 0.9229593570900161\n",
      "Test f1: 0.9116246699164265\n",
      "End of epoch 28, Loss 0.19713045374213994\n",
      "Train accuracy: 0.9208830856455631\n",
      "Test accuracy: 0.9216763998625902\n",
      "Test precision: 0.8948145303688972\n",
      "Test recall: 0.9183931013961628\n",
      "Test f1: 0.9053500735856159\n",
      "End of epoch 29, Loss 0.19267749567362633\n",
      "Train accuracy: 0.9212266987372218\n",
      "Test accuracy: 0.9230504981106149\n",
      "Test precision: 0.9063548052381747\n",
      "Test recall: 0.9007158407026565\n",
      "Test f1: 0.9034722880151587\n",
      "End of epoch 30, Loss 0.19224871478074199\n",
      "Train accuracy: 0.9229447641955159\n",
      "Test accuracy: 0.9151494331844727\n",
      "Test precision: 0.8843119560228379\n",
      "Test recall: 0.9214771616648574\n",
      "Test f1: 0.8994581636292982\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(res_net_saved_recall, hb_train_loader2, criterion, optimizer_res_net, hb_test_loader2, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
